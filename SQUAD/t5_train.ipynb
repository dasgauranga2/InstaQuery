{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import numpy as np\n",
    "from torchtext import data\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "from transformers import T5Model,T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Hello', '▁world', '▁how', '▁are', '▁you', '?']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize('Hello world how are you?')\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8774, 296, 149, 33, 25, 58]\n"
     ]
    }
   ],
   "source": [
    "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> </s> <pad> <unk>\n"
     ]
    }
   ],
   "source": [
    "init_token = tokenizer.pad_token\n",
    "eos_token = tokenizer.eos_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 0 2\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes['t5-small']\n",
    "\n",
    "print(max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauranga/anaconda3/envs/pt/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "SRC = Field(batch_first = True,\n",
    "          use_vocab = False,\n",
    "          tokenize = tokenize_and_cut,\n",
    "          preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "          init_token = init_token_idx,\n",
    "          eos_token = eos_token_idx,\n",
    "          pad_token = pad_token_idx,\n",
    "          unk_token = unk_token_idx)\n",
    "\n",
    "TRG = Field(batch_first = True,\n",
    "          use_vocab = False,\n",
    "          tokenize = tokenize_and_cut,\n",
    "          preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "          init_token = init_token_idx,\n",
    "          eos_token = eos_token_idx,\n",
    "          pad_token = pad_token_idx,\n",
    "          unk_token = unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('src', SRC), ('trg', TRG)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauranga/anaconda3/envs/pt/lib/python3.8/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/home/gauranga/anaconda3/envs/pt/lib/python3.8/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_data = data.TabularDataset.splits(\n",
    "                path = '',\n",
    "                train = 'squad.csv',\n",
    "                format = 'csv',\n",
    "                fields = fields,\n",
    "                skip_header = True)\n",
    "\n",
    "train_data , valid_data = train_data[0].split(split_ratio=0.98,\n",
    "                                             random_state = random.seed(4321))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57226\n",
      "1168\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data.examples))\n",
    "print(len(valid_data.examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': [2625, 3, 10, 16, 16433, 6, 3, 12687, 15, 157, 47, 2404, 13, 8, 3, 6856, 18, 9339, 3, 5379, 17, 6, 16, 84, 8, 3, 2160, 17, 1273, 10215, 1088, 4686, 12, 453, 8, 3, 2160, 17, 1273, 12568, 789, 16, 828, 5, 913, 12, 8, 648, 6, 3, 12687, 15, 157, 243, 6, 96, 13726, 80, 113, 65, 3, 12895, 3, 9, 508, 294, 13, 112, 280, 12, 8, 810, 13, 8, 892, 11, 8, 5559, 13, 5486, 6835, 500, 91, 24, 3, 9, 1088, 24, 5689, 3, 9, 23737, 789, 16, 579, 65, 1513, 66, 2233, 12, 8, 564, 3, 31, 10661, 138, 31, 5, 822, 3, 10, 125, 410, 3, 12687, 15, 157, 857, 47, 1513, 16, 8, 2925, 26518, 13, 3, 9, 23737, 789, 58], 'trg': [66, 2233, 12, 8, 564, 3, 31, 10661, 138, 31]}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[25000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁context', '▁', ':', '▁in', '▁1957', ',', '▁bra', 'un', 'stein', '▁further', '▁demonstrated', '▁that', '▁the', '▁', 'r', 'udi', 'ment', 'ary', '▁devices', '▁could', '▁be', '▁used', '▁for', '▁non', '-', 'radi', 'o', '▁communication', '▁across', '▁', 'a', '▁short', '▁distance', '.', '▁question', '▁', ':', '▁what', '▁year', '▁was', '▁it', '▁discovered', '▁that', '▁early', '▁led', '▁instruments', '▁could', '▁be', '▁used', '▁for', '▁non', '-', 'radi', 'o', '▁communication', '?']\n",
      "['▁1957']\n"
     ]
    }
   ],
   "source": [
    "src_tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[2000])['src'])\n",
    "trg_tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[2000])['trg'])\n",
    "\n",
    "print(src_tokens)\n",
    "print(trg_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauranga/anaconda3/envs/pt/lib/python3.8/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "BATCH_SIZE = 24\n",
    "\n",
    "train_iterator, valid_iterator = BucketIterator.splits(\n",
    "                                 (train_data, valid_data), \n",
    "                                 batch_size = BATCH_SIZE,\n",
    "                                 device = device,\n",
    "                                 sort_key=lambda x: len(x.src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.t5 = T5Model.from_pretrained('t5-small')\n",
    "        \n",
    "        self.out = nn.Linear(self.t5.config.to_dict()['d_model'],\n",
    "                             self.t5.config.to_dict()['vocab_size'])\n",
    "                \n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        embedded = self.t5(input_ids=src,decoder_input_ids=trg) \n",
    "        \n",
    "        output = self.out(embedded[0])\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5Model were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = T5Network().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 76,988,544 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0004\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = pad_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 4\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauranga/anaconda3/envs/pt/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 1\tTRAIN LOSS : 2.66\tVALID LOSS : 0.90\tTIME : 1027.51\n",
      "\n",
      "EPOCH : 2\tTRAIN LOSS : 0.67\tVALID LOSS : 0.60\tTIME : 1042.50\n",
      "\n",
      "EPOCH : 3\tTRAIN LOSS : 0.38\tVALID LOSS : 0.53\tTIME : 1045.49\n",
      "\n",
      "EPOCH : 4\tTRAIN LOSS : 0.27\tVALID LOSS : 0.53\tTIME : 1050.51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # TRAINING\n",
    "    ##############################################################################\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(train_iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg[:,:-1])\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    train_loss = epoch_loss / len(train_iterator)\n",
    "    ##############################################################################\n",
    "    \n",
    "    # VALIDATION\n",
    "    ##############################################################################\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(valid_iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg[:,:-1])\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    valid_loss = epoch_loss / len(valid_iterator)\n",
    "    ##############################################################################\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"EPOCH : {epoch+1}\\tTRAIN LOSS : {train_loss:.2f}\\tVALID LOSS : {valid_loss:.2f}\\tTIME : {end_time-start_time:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT ALL MODEL WEIGHTS AND BIASES TO HALF PRECISION\n",
    "# MODEL SIZE WILL REDUCE\n",
    "model = model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 't5_qa_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence2(sentence, eval_model, device, max_len = 50):\n",
    "    \n",
    "    eval_model.eval()\n",
    "\n",
    "    src_indexes = [init_token_idx] + sentence + [eos_token_idx]\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "    trg_indexes = [init_token_idx]\n",
    "\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = eval_model(src_tensor, trg_tensor)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == eos_token_idx:\n",
    "            break\n",
    "\n",
    "    return trg_indexes[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC : ▁context▁:▁in▁1979,▁the▁japanese▁state▁broadcaster▁nhk▁first▁developed▁consumer▁high-definition▁television▁with▁a▁5:3▁display▁aspect▁ratio.▁question▁:▁in▁what▁year▁did▁nhk▁first▁develop▁consumer▁hd▁television▁with▁a▁5:3▁aspect▁ratio?\n",
      "TRG : ▁1979\n",
      "PRED : ▁1979</s>\n",
      "\n",
      "SRC : ▁context▁:▁the▁company▁aims▁to▁reduce▁its▁external▁environmental▁impact▁through▁energy-efficient▁evolution▁of▁products,▁and▁also▁reduce▁its▁direct▁operational▁impact▁through▁energy-efficiency▁programs.▁internal▁energy-efficiency▁programs▁reportedly▁save▁the▁company▁more▁than▁$3▁million▁annually▁in▁energy-cost▁savings.▁the▁largest▁component▁of▁the▁company's▁internal▁energy-efficiency▁savings▁comes▁through▁pc▁power▁management:▁the▁company▁expects▁to▁save▁$1.8▁million▁in▁energy▁costs▁through▁using▁specialized▁energy-management▁software▁on▁a▁network▁of▁50,000▁pcs.▁question▁:▁what▁does▁dell▁aim▁to▁reduce▁by▁creating▁energy▁efficient▁products?\n",
      "TRG : ▁external▁environmental▁impact\n",
      "PRED : ▁external▁environmental▁impact</s>\n",
      "\n",
      "SRC : ▁context▁:▁throughout▁her▁career▁madonna▁has▁been▁involved▁in▁writing▁and▁producing▁most▁of▁her▁own▁music.▁question▁:▁who▁writes▁and▁creates▁most▁of▁her▁own▁music?\n",
      "TRG : ▁madonna\n",
      "PRED : ▁madonna</s>\n",
      "\n",
      "SRC : ▁context▁:▁mexico▁city,▁being▁the▁seat▁of▁the▁powers▁of▁the▁union,▁did▁not▁belong▁to▁any▁particular▁state▁but▁to▁all.▁therefore,▁it▁was▁the▁president,▁representing▁the▁federation,▁who▁used▁to▁designate▁the▁head▁of▁government▁of▁the▁federal▁district,▁a▁position▁which▁is▁sometimes▁presented▁outside▁mexico▁as▁the▁\"mayor\"▁of▁mexico▁city.▁question▁:▁who▁declared▁the▁\"mayor\"▁of▁mexico▁city?\n",
      "TRG : ▁the▁president\n",
      "PRED : ▁president</s>\n",
      "\n",
      "SRC : ▁context▁:▁in▁1985,▁schwarzenegger▁appeared▁in▁\"stop▁the▁madness\",▁an▁anti-drug▁music▁video▁sponsored▁by▁the▁reagan▁administration.▁he▁first▁came▁to▁wide▁public▁notice▁as▁a▁republican▁during▁the▁1988▁presidential▁election,▁accompanying▁then-vice▁president▁george▁h.w.▁bush▁at▁a▁campaign▁rally.▁question▁:▁in▁what▁presidential▁election▁year▁did▁schwarzenegger▁make▁a▁name▁for▁himself▁as▁a▁prominent▁republican?\n",
      "TRG : ▁1988\n",
      "PRED : ▁1988</s>\n",
      "\n",
      "SRC : ▁context▁:▁southampton▁has▁always▁been▁a▁port,▁and▁the▁docks▁have▁long▁been▁a▁major▁employer▁in▁the▁city.▁in▁particular,▁it▁is▁a▁port▁for▁cruise▁ships;▁its▁heyday▁was▁the▁first▁half▁of▁the▁20th▁century,▁and▁in▁particular▁the▁inter-war▁years,▁when▁it▁handled▁almost▁half▁the▁passenger▁traffic▁of▁the▁uk.▁question▁:▁about▁how▁much▁of▁uk's▁passenger▁traffic▁did▁southampton▁handle▁during▁the▁inter-war▁period?\n",
      "TRG : ▁half\n",
      "PRED : ▁half</s>\n",
      "\n",
      "SRC : ▁context▁:▁in▁2002,▁spielberg▁was▁one▁of▁eight▁flagbearers▁who▁carried▁the▁olympic▁flag▁into▁rice-eccles▁stadium▁at▁the▁opening▁ceremonies▁of▁the▁2002▁winter▁olympic▁games▁in▁salt▁lake▁city.▁question▁:▁where▁was▁spielberg▁an▁olympic▁flagbearer?\n",
      "TRG : ▁salt▁lake▁city\n",
      "PRED : ▁salt▁lake▁city</s>\n",
      "\n",
      "SRC : ▁context▁:▁by▁the▁1860s▁the▁kingdom▁of▁prussia▁and▁the▁austrian▁empire▁were▁the▁two▁most▁powerful▁nations▁dominated▁by▁german-speaking▁elites.▁question▁:▁who▁were▁the▁two▁most▁powerful▁nations▁in▁the▁1860's?\n",
      "TRG : ▁kingdom▁of▁prussia▁and▁the▁austrian▁empire\n",
      "PRED : ▁kingdom▁of▁prussia▁and▁the▁austrian▁empire</s>\n",
      "\n",
      "SRC : ▁context▁:▁in▁april▁2009,▁the▁united▁states▁supreme▁court▁agreed▁to▁hear▁a▁suit▁over▁reverse▁discrimination▁brought▁by▁20▁white▁and▁hispanic▁firefighters▁against▁the▁city.▁the▁suit▁involved▁the▁2003▁promotion▁test▁for▁the▁new▁haven▁fire▁department.▁after▁the▁tests▁were▁scored,▁no▁blacks▁scored▁high▁enough▁to▁qualify▁for▁consideration▁for▁promotion,▁so▁the▁city▁announced▁that▁no▁one▁would▁be▁promoted.▁on▁29▁june▁2009,▁the▁united▁states▁supreme▁court▁ruled▁in▁favor▁of▁the▁firefighters,▁agreeing▁that▁they▁were▁improperly▁denied▁promotion▁because▁of▁their▁race.▁question▁:▁what▁was▁the▁date▁of▁the▁final▁ruling?\n",
      "TRG : ▁29▁june▁2009\n",
      "PRED : ▁29▁june</s>\n",
      "\n",
      "SRC : ▁context▁:▁somalis▁constitute▁the▁largest▁ethnic▁group▁in▁somalia,▁at▁approximately▁85%▁of▁the▁nation's▁inhabitants.▁they▁are▁traditionally▁nomads,▁but▁since▁the▁late▁20th▁century,▁many▁have▁moved▁to▁urban▁areas.▁while▁most▁somalis▁can▁be▁found▁in▁somalia▁proper,▁large▁numbers▁also▁live▁in▁ethiopia,▁djibouti,▁kenya,▁yemen,▁the▁middle▁east,▁south▁asia▁and▁europe▁due▁to▁their▁seafaring▁tradition.▁question▁:▁why▁do▁many▁somalis▁live▁in▁south▁asia▁and▁europe?\n",
      "TRG : ▁their▁seafaring▁tradition\n",
      "PRED : ▁their▁seafaring▁tradition</s>\n",
      "\n",
      "SRC : ▁context▁:▁nakamura▁was▁awarded▁the▁2006▁millennium▁technology▁prize▁for▁his▁invention.▁nakamura,▁hiroshi▁amano▁and▁isamu▁akasaki▁were▁awarded▁the▁nobel▁prize▁in▁physics▁in▁2014▁for▁the▁invention▁of▁the▁blue▁led.▁question▁:▁what▁nobel▁prize▁did▁nakamura,▁amano,▁and▁akasaki▁receive▁in▁2014?\n",
      "TRG : ▁physics\n",
      "PRED : ▁the▁nobel▁prize▁in▁physics</s>\n",
      "\n",
      "SRC : ▁context▁:▁snapshots▁usually▁become▁available▁more▁than▁six▁months▁after▁they▁are▁archived▁or,▁in▁some▁cases,▁even▁later;▁it▁can▁take▁twenty-four▁months▁or▁longer.▁the▁frequency▁of▁snapshots▁is▁variable,▁so▁not▁all▁tracked▁web▁site▁updates▁are▁recorded.▁sometimes▁there▁are▁intervals▁of▁several▁weeks▁or▁years▁between▁snapshots.▁question▁:▁what▁term▁characterizes▁the▁rate▁at▁which▁snapshots▁are▁made▁of▁websites?\n",
      "TRG : ▁variable\n",
      "PRED : ▁variable</s>\n",
      "\n",
      "SRC : ▁context▁:▁this▁liberalization,▁however,▁fostered▁nationalist▁movements▁and▁ethnic▁disputes▁within▁the▁soviet▁union.▁it▁also▁led▁indirectly▁to▁the▁revolutions▁of▁1989,▁in▁which▁soviet-imposed▁communist▁regimes▁of▁the▁warsaw▁pact▁were▁peacefully▁toppled▁(romania▁excepted),▁which▁in▁turn▁increased▁pressure▁on▁gorbachev▁to▁introduce▁greater▁democracy▁and▁autonomy▁for▁the▁soviet▁union's▁constituent▁republics.▁question▁:▁in▁which▁country▁did▁warsaw▁pact▁regime▁remain▁in▁place?\n",
      "TRG : ▁romania\n",
      "PRED : ▁republicania</s>\n",
      "\n",
      "SRC : ▁context▁:▁according▁to▁the▁u.s.▁census▁bureau,▁as▁of▁2015,▁tennessee▁had▁an▁estimated▁population▁of▁6,600,299,▁which▁is▁an▁increase▁of▁50,947,▁from▁the▁prior▁year▁and▁an▁increase▁of▁254,194,▁or▁4.▁question▁:▁what▁was▁tennessee's▁estimated▁population▁in▁2015?\n",
      "TRG : ▁6,600,299\n",
      "PRED : ▁6,600,299</s>\n",
      "\n",
      "SRC : ▁context▁:▁congo's▁democratic▁progress▁was▁derailed▁in▁1997▁when▁lissouba▁and▁sassou▁started▁to▁fight▁for▁power▁in▁the▁civil▁war.▁question▁:▁which▁two▁figures▁clashed▁over▁leadership▁of▁the▁congo▁in▁1997?\n",
      "TRG : ▁lissouba▁and▁sassou\n",
      "PRED : ▁lissouba▁and▁sassou</s>\n",
      "\n",
      "SRC : ▁context▁:▁european▁overseas▁expansion▁led▁to▁the▁rise▁of▁colonial▁empires,▁producing▁the▁columbian▁exchange.▁the▁combination▁of▁resource▁inflows▁from▁the▁new▁world▁and▁the▁industrial▁revolution▁of▁great▁britain,▁allowed▁a▁new▁economy▁based▁on▁manufacturing▁instead▁of▁subsistence▁agriculture.▁question▁:▁what▁replaced▁agriculture▁as▁the▁focus▁of▁european▁economy▁with▁the▁establishment▁of▁overseas▁colonies?\n",
      "TRG : ▁manufacturing\n",
      "PRED : ▁manufacturing</s>\n",
      "\n",
      "SRC : ▁context▁:▁formally,▁a▁\"database\"▁refers▁to▁a▁set▁of▁related▁data▁and▁the▁way▁it▁is▁organized.▁access▁to▁these▁data▁is▁usually▁provided▁by▁a▁\"database▁management▁system\"▁(dbms)▁consisting▁of▁an▁integrated▁set▁of▁computer▁software▁that▁allows▁users▁to▁interact▁with▁one▁or▁more▁databases▁and▁provides▁access▁to▁all▁of▁the▁data▁contained▁in▁the▁database▁(although▁restrictions▁may▁exist▁that▁limit▁access▁to▁particular▁data).▁question▁:▁a▁dbms▁consists▁of▁what?\n",
      "TRG : ▁an▁integrated▁set▁of▁computer▁software\n",
      "PRED : ▁an▁integrated▁set▁of▁computer▁software</s>\n",
      "\n",
      "SRC : ▁context▁:▁beyoncé▁giselle▁knowles-carter▁(/bi<unk>j<unk>nse<unk>/▁bee-yon-say)▁(born▁september▁4,▁1981)▁is▁an▁american▁singer,▁songwriter,▁record▁producer▁and▁actress.▁born▁and▁raised▁in▁houston,▁texas,▁she▁performed▁in▁various▁singing▁and▁dancing▁competitions▁as▁a▁child,▁and▁rose▁to▁fame▁in▁the▁late▁1990s▁as▁lead▁singer▁of▁r&b▁girl-group▁destiny's▁child.▁question▁:▁what▁areas▁did▁beyonce▁compete▁in▁when▁she▁was▁growing▁up?\n",
      "TRG : ▁singing▁and▁dancing\n",
      "PRED : ▁sing▁and▁dancing</s>\n",
      "\n",
      "SRC : ▁context▁:▁in▁1953,▁the▁republican▁party's▁old▁guard▁presented▁eisenhower▁with▁a▁dilemma▁by▁insisting▁he▁disavow▁the▁yalta▁agreements▁as▁beyond▁the▁constitutional▁authority▁of▁the▁executive▁branch;▁however,▁the▁death▁of▁joseph▁stalin▁in▁march▁1953▁made▁the▁matter▁a▁practical▁moot▁point.▁question▁:▁why▁did▁the▁old▁guard▁say▁eisenhower▁should▁void▁the▁yalta▁agreements?\n",
      "TRG : ▁beyond▁the▁constitutional▁authority▁of▁the▁executive▁branch\n",
      "PRED : ▁beyond▁the▁constitutional▁authority▁of▁the▁executive▁branch</s>\n",
      "\n",
      "SRC : ▁context▁:▁in▁the▁mid▁1970s,▁various▁american▁groups▁(some▁with▁ties▁to▁downtown▁manhattan's▁punk▁scene,▁including▁television▁and▁suicide)▁had▁begun▁expanding▁on▁the▁vocabulary▁of▁punk▁music.▁question▁:▁which▁american▁bands▁had▁ties▁to▁the▁manchester▁punk▁scene?\n",
      "TRG : ▁television▁and▁suicide\n",
      "PRED : ▁television▁and▁suicide</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idxs = random.sample(range(0, len(valid_data.examples)), 20)\n",
    "for i in idxs:\n",
    "    src = vars(valid_data.examples[i])['src']\n",
    "    trg = vars(valid_data.examples[i])['trg']\n",
    "    translation = translate_sentence2(src, model, device)\n",
    "\n",
    "    print(f\"SRC : {''.join(tokenizer.convert_ids_to_tokens(src))}\")\n",
    "    print(f\"TRG : {''.join(tokenizer.convert_ids_to_tokens(trg))}\")\n",
    "    print(f\"PRED : {''.join(tokenizer.convert_ids_to_tokens(translation))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
